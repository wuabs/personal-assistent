from dotenv import load_dotenv
load_dotenv()

import gradio as gr
from langchain.agents import initialize_agent, AgentType
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEndpoint
from langchain.memory import ConversationBufferMemory
from tools import get_tools

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
chat_history = []
agent = None

# –§—É–Ω–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
def get_llm(model_choice):
    if model_choice == "OpenAI GPT":
        return ChatOpenAI(model="gpt-4o", temperature=0.4)
    elif model_choice == "HuggingFace: flan-t5-xl":
        return HuggingFaceEndpoint(repo_id="google/flan-t5-xl", temperature=0.4)
    elif model_choice == "HuggingFace: Mistral-7B":
        return HuggingFaceEndpoint(repo_id="mistralai/Mistral-7B-Instruct-v0.1", temperature=0.4)
    return ChatOpenAI(temperature=0.4)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
def init_agent(model_choice):
    global agent, memory, chat_history
    memory.clear()
    chat_history = []
    llm = get_llm(model_choice)
    tools = get_tools(llm)
    agent = initialize_agent(
        tools=tools,
        llm=llm,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        memory=memory,
        verbose=False
    )
    return f"–ú–æ–¥–µ–ª—å '{model_choice}' –∑–∞–≥—Ä—É–∂–µ–Ω–∞!"

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—â–µ–Ω–∏—è
def chat_with_assistant(user_input, file):
    global chat_history
    if agent is None:
        return "‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å!", ""

    try:
        if file is not None:
            with open(file.name, "r", encoding="utf-8") as f:
                content = f.read()
            query = f"–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ –æ–±–æ–±—â–µ–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n\n{content}"
            result = agent.invoke(query)
        else:
            result = agent.invoke(user_input)

        if result is None:
            result = "‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç"

    except StopIteration:
        result = "‚ö†Ô∏è –û—à–∏–±–∫–∞: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∏–ª–∞—Å—å –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ (StopIteration)"
    except Exception as e:
        result = f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {str(e)}"

    if user_input or file:
        chat_history.append(f"üßë {user_input if user_input else '[–∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª]'}")
        chat_history.append(f"ü§ñ {result}")
    return result, "\n\n".join(chat_history)


# –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏
def clear_history():
    memory.clear()
    chat_history.clear()
    return "", ""

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
with gr.Blocks() as iface:
    gr.Markdown("### ü§ñ –õ–∏—á–Ω—ã–π AI-–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")

    model_choice = gr.Dropdown(
        choices=[
            "OpenAI GPT",
            "HuggingFace: flan-t5-xl",
            "HuggingFace: Mistral-7B"
        ],
        value="OpenAI GPT",
        label="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å"
    )
    load_btn = gr.Button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")

    with gr.Row():
        user_input = gr.Textbox(lines=3, placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å...", label="–í–∞—à –∑–∞–ø—Ä–æ—Å")
        file_input = gr.File(label="–ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ .txt —Ñ–∞–π–ª", file_types=[".txt"])

    output = gr.Textbox(label="–û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    history = gr.Textbox(label="üïò –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞", lines=10)

    submit_btn = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å")
    clear_btn = gr.Button("üßπ –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")

    load_btn.click(init_agent, inputs=[model_choice], outputs=[output])
    submit_btn.click(chat_with_assistant, inputs=[user_input, file_input], outputs=[output, history])
    clear_btn.click(clear_history, outputs=[output, history])

if __name__ == "__main__":
    iface.launch()
